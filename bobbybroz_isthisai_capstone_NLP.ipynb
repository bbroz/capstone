{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Spring 2019: Is this AI?\n",
    "by: bobby broz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Abstract/Motivation:__ <br>\n",
    "\n",
    "Professionals looking to improve their interpersonal communication skills or companies looking to\n",
    "help improve their transaction success rates with their customers. This capstone seeks to create a\n",
    "machine learning approach that's designed to identify and record any questions asked during\n",
    "conversation. Using NLP to analyze all text queries and return them to the user (or digital\n",
    "assistant) to aid them in preparing an appropriate response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP\n",
    "\n",
    "Dataset: https://www.kaggle.com/bryanpark/the-world-english-bible-speech-dataset <br>\n",
    "\n",
    "This dataset is composed of the following: <br>\n",
    "- README.md\n",
    "- wav files sampled at 12,000 KHZ\n",
    "- transcript.txt.\n",
    "<br>\n",
    "\n",
    "`transcript.txt` is in a tab-delimited format. <br>\n",
    "\n",
    "The first column is the name of the `book` and audio file paths. <br> \n",
    "The second one is the `script`. <br>\n",
    "The rightmost column is the `duration` of the audio file. <br>\n",
    "\n",
    "This is a supervised learning classification problem. <br>\n",
    "\n",
    "The goal here is to obtain the text and load it into a DataFrame and be able to parse and recognize what is a question based on a its denotation. <br>\n",
    "\n",
    "eg. who? what? where? why? how? <br>\n",
    "\n",
    "Going deeper, you could look at Does..? Will..? Am I..? Which is..? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages necessary for storing, manipulating and displaying text data statistics\n",
    "\n",
    "from tqdm import tqdm # used for tracking execution time of code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rc('figure', figsize=(16, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and display tab delimited values, assign column names\n",
    "transcript_txt_df = pd.read_csv('transcript.txt',sep='\\t',header=None)\n",
    "transcript_txt_df.columns=['book','script','duration']\n",
    "transcript_txt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaN values\n",
    "transcript_txt_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate NaN values\n",
    "display(transcript_txt_df.loc[transcript_txt_df['script'].isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Both the audio and script data for the verses above were removed. Upon further investigation, it was determined that these two verses are routinely removed in the modern day bible as they could be negatively misinterpretted. A decision was made to simply put in the word 'BRAINSTATION' as one unknown word from the 21st century will not affect the accuracy of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transcript_txt_df['script'][25047] = 'BRAINSTATION'\n",
    "transcript_txt_df['script'][25565] = 'BRAINSTATION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm NaN values were removed\n",
    "transcript_txt_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if all 31,083 lines were correctly read in\n",
    "print(len(transcript_txt_df), 'lines in DataFrame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ `transcript.txt` has 31083 lines. <br>\n",
    "\n",
    "28825/31083 = 0.927355789338223 <br>\n",
    "Approximately ~8% of data has not been read in correctly. <br>\n",
    "\n",
    "__Data Cleaning required.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# count number of aggregated and mis-read lines in DataFrame\n",
    "misread_count = transcript_txt_df['script'].str.find(\"\\n\")\n",
    "print(misread_count.gt(0).sum(),\"rows incorrectly read in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more exploratory data analysis\n",
    "\n",
    "A List of all the books, chapters and verses in the bible:\n",
    "https://www.blueletterbible.org/study/misc/66books.cfm <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruth should have: <br> \n",
    "- 4 books <br> \n",
    "- 85 verses <br>\n",
    "\n",
    "and the DataFrame matches this expectation <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(transcript_txt_df.loc[transcript_txt_df['book'].str.contains('Ruth')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leviticus should have: <br> \n",
    "- 27 books <br> \n",
    "- 859 verses <br>\n",
    "\n",
    "and the DataFrame __does not__ match this expection. <br>\n",
    "The data was likely not read in correctly. Further EDA required. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(transcript_txt_df.loc[transcript_txt_df['book'].str.contains('Leviticus')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EDA__ <br>\n",
    "From above we see that `Leviticus/Leviticus_2-1` jumps 3 books to `Leviticus/Leviticus_2-5` <br>\n",
    "However their respectives rows increase by one `2495` -> `2496` <br>\n",
    "Need to investigate further, display book title and script. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display known row that was mis-read incorrectly\n",
    "display(transcript_txt_df['book'].iloc[2495])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_txt_df['script'].iloc[2495]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EDA__ <br>\n",
    "We can see that this contains several `\\t` and `\\n` delimiters, separating the book titles, scripts and durations for the following books: <br>\n",
    "- `Leviticus/Leviticus_2-2` <br>\n",
    "- `Leviticus/Leviticus_2-3` <br>\n",
    "- `Leviticus/Leviticus_2-4` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pre-processing Pseudo__ <br>\n",
    "\n",
    "1. Create `book[]`,`script[]`,`duration[]` master lists <br>\n",
    "__Main Loop: Steps 2-7__ through `transcript_txt_df` DataFrame <br>\n",
    "2. Read single row and store into `string[]`. Check `string[]` for `\\n`. <br>\n",
    "3. If True, append row values from `transcript_txt_df` from respective columns into `book[]`,`script[]`,`duration[]` master lists <br>\n",
    "4. If False, split() `string[]` by the `\\n` delimiter and store into `substring[]` <br>\n",
    "__Sub Loop: Steps 5-7__ <br>\n",
    "5. Sub-loop through `substring[]` list and split() each pass by the `\\t` delimiter and store the 3 outputs into `book_temp`,`script_temp`,`duration_temp` <br>\n",
    "6. Append `book_temp`,`script_temp`,`duration_temp` to `book[]`,`script[]`,`duration[]` master lists <br>\n",
    "7. Once sub-loop scans entire length of `substring[]`, loop finishes and goes back to main loop <br>\n",
    "\n",
    "This continues until entire `transcript_txt_df` DataFrame is scanned. The result is 3 separate master lists: \n",
    "\n",
    "- `book[]`\n",
    "- `script[]`\n",
    "- `duration[]`\n",
    "\n",
    "These master lists have 31083 lines and will be combined into a `transcript_clean_df` DataFrame with each list as it's respectively named column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "book = []\n",
    "script = []\n",
    "duration = []\n",
    "\n",
    "tab = \"\\t\"\n",
    "newline = \"\\n\"\n",
    "\n",
    "num_correct_lines = 0\n",
    "num_fixed_lines = 0\n",
    "\n",
    "# print(f'Transcript Lines read in: {len(transcript_txt_df)}')\n",
    "\n",
    "for x in tqdm( range(0,len(transcript_txt_df)) ):\n",
    "\n",
    "    string = transcript_txt_df['book'].iloc[x] +\"\\t\"+ transcript_txt_df['script'].iloc[x]+\"\\t\"+str(transcript_txt_df['duration'].iloc[x])\n",
    "    # print(f\"transcript_df['book'].iloc[{x}] + transcript_df['script'].iloc[{x}] + str(transcript_df['duration'].iloc[{x}] = \")\n",
    "    # display(string)\n",
    "\n",
    "    if (string.find(\"\\n\") < 0):\n",
    "\n",
    "        book.append(transcript_txt_df['book'].iloc[x])\n",
    "        script.append(transcript_txt_df['script'].iloc[x])\n",
    "        duration.append(str(transcript_txt_df['duration'].iloc[x]))\n",
    "        num_correct_lines += 1\n",
    "\n",
    "    else:\n",
    "\n",
    "        substring = string.split(newline)\n",
    "\n",
    "        # print('substring = ')\n",
    "        # display(substring)\n",
    "\n",
    "        for y in range(0,len(substring)):\n",
    "    #         print(y)\n",
    "    #         display(substring[y].split(tab))\n",
    "            book_temp,script_temp,duration_temp = substring[y].split(tab)\n",
    "            book.append(book_temp)\n",
    "            script.append(script_temp)\n",
    "            duration.append(duration_temp) \n",
    "            num_fixed_lines += 1\n",
    "            \n",
    "#     if (x%5000==0): print('Line',x,'of',len(transcript_txt_df))\n",
    "\n",
    "print(f'Scanned {x} lines!')\n",
    "print(f'Number of initially scanned lines = {num_correct_lines}')\n",
    "print(f'Number of fixed lines =              {num_fixed_lines}')\n",
    "\n",
    "print(f\"Length of book list     =           {len(book)}\")\n",
    "print(f\"Length of script list   =           {len(script)}\")\n",
    "print(f\"Length of duration list =           {len(duration)}\")\n",
    "\n",
    "print(f'Total lines =                       {num_correct_lines+num_fixed_lines}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame, load each respective list into column and name it\n",
    "transcript_clean_df = pd.DataFrame()\n",
    "transcript_clean_df['book'] = book\n",
    "transcript_clean_df['script'] = script\n",
    "transcript_clean_df['duration'] = duration\n",
    "transcript_clean_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify which lines in `transcript_clean_df` DataFrame have questions and store it in `target_df` DataFrame  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target variable (question identifier)\n",
    "question = []\n",
    "for z in range(0,len(transcript_clean_df)):\n",
    "    question.append(transcript_clean_df['script'][z].count('?'))\n",
    "    \n",
    "print(f'Scanned {z} lines!')\n",
    "\n",
    "target_df = pd.DataFrame(question)\n",
    "target_df.columns = ['question']\n",
    "len(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if line 66 has two questions\n",
    "display(target_df.head())\n",
    "display(f\"line 66 has {target_df['question'].iloc[66]} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count total number of questions\n",
    "target_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show most number of questions in DataFrame for first 300 rows\n",
    "target_df['question'][:300].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify row 85 from above, result should show 2 questions in line\n",
    "transcript_clean_df['script'].iloc[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_clean_df['script'].iloc[21401]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `feature_df` by copying `transcript_clean_df`. This feature DataFrame will have all punctuation removed from the script and be brought down to lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = transcript_clean_df.copy()\n",
    "feature_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all punctuation and convert each string into lower case for all 31,083 verses\n",
    "\n",
    "import string\n",
    "\n",
    "print(string.punctuation)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "feature_df['script'] = feature_df['script'].apply(remove_punctuation)\n",
    "display(feature_df['script'].head())\n",
    "\n",
    "for m in tqdm( range(0,len(feature_df)) ):\n",
    "    feature_df['script'][m] = str(feature_df['script'][m]).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['script'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "print(len(str(ENGLISH_STOP_WORDS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine feature and target into one DataFrame\n",
    "# create a train and test set for dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "web_df = feature_df.copy()\n",
    "web_df['target'] = target_df.copy()\n",
    "web_df['target'].values[web_df['target'] > 0] = 1\n",
    "\n",
    "X = web_df['script']\n",
    "y = web_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,stratify=y, random_state=1)\n",
    "\n",
    "# vectorize all words using term frequency, inverse document frequency\n",
    "# Create document-term matrix using TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf.fit(X_train)\n",
    "X_train = tfidf.transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Given that there are only 3241 questions out of 31083 verses, we need to balance the two classes (question/non-question) so that the algorithms are not trained on a imbalanced/biased dataset. This will be doing using Synthetic Minority Over-sampling TEchnique (SMOTE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the SMOTE package, we balance the question/non-question classes using a 1:1 ratio\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# instantiate\n",
    "smote = SMOTE(ratio=1.0)\n",
    "# fit\n",
    "X_train_smote, y_train_smote = smote.fit_sample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "\n",
    "- logistic regression\n",
    "- naive bayes\n",
    "- random forest\n",
    "- scaled vector machine\n",
    "- xgboost\n",
    "- neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate\n",
    "logit_smote = LogisticRegression(solver='lbfgs')\n",
    "logit = LogisticRegression(solver='lbfgs')\n",
    "logit_weight = LogisticRegression(solver='lbfgs',class_weight='balanced')\n",
    "\n",
    "# fit\n",
    "logit_smote.fit(X_train_smote,y_train_smote)\n",
    "logit.fit(X_train,y_train)\n",
    "logit_weight.fit(X_train,y_train)\n",
    "\n",
    "# predict\n",
    "y_pred_smote = logit_smote.predict(X_test)\n",
    "y_pred = logit.predict(X_test)\n",
    "y_pred_weight = logit_weight.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('Logit Train Score:      ',logit.score(X_train,y_train))\n",
    "print('Logit Test Score:       ',logit.score(X_test,y_test))\n",
    "print('Logit Train Smote Score:',logit_smote.score(X_train_smote,y_train_smote))\n",
    "print('Logit Test Smote Score: ',logit_smote.score(X_test,y_test))\n",
    "print('Logit Balanced Train Score:      ',logit_weight.score(X_train,y_train))\n",
    "print('Logit Balanced Test Score:       ',logit_weight.score(X_test,y_test))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('y_pred_smote:\\n', classification_report(y_test, y_pred_smote))\n",
    "print('y_pred_smote confusion matrix:\\n',confusion_matrix(y_test, y_pred_smote),'\\n')\n",
    "print('y_pred:\\n', classification_report(y_test, y_pred))\n",
    "print('y_pred confusion matrix:\\n',confusion_matrix(y_test, y_pred),'\\n')\n",
    "print('y_pred_weight:\\n', classification_report(y_test, y_pred_weight))\n",
    "print('y_pred_weight confusion matrix:\\n',confusion_matrix(y_test, y_pred_weight),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# convert from sparse matrix to array \n",
    "\n",
    "X_train_gnb = X_train.todense()\n",
    "X_train_gnb_smote = X_train_smote.todense()\n",
    "\n",
    "# instantiate\n",
    "gnb = GaussianNB()\n",
    "gnb_smote = GaussianNB()\n",
    "\n",
    "# fit\n",
    "gnb.fit(X_train_gnb,y_train)\n",
    "gnb_smote.fit(X_train_gnb_smote,y_train_smote)\n",
    "\n",
    "# predict\n",
    "y_pred_gnb = gnb.predict(X_test.todense())\n",
    "y_pred_gnb_smote = gnb_smote.predict(X_test.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gnb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Naive Bayes Train Score:      ',gnb.score(X_train_gnb,y_train))\n",
    "print('Naive Bayes Test Score:       ',gnb.score(X_test.todense(),y_test))\n",
    "print('Naive Bayes Smote Train Score:',gnb_smote.score(X_train_gnb_smote,y_train_smote))\n",
    "print('Naive Bayes Smote Test Score: ',gnb_smote.score(X_test.todense(),y_test))\n",
    "print('\\n')\n",
    "print('y_pred_gnb:\\n', classification_report(y_test, y_pred_gnb))\n",
    "print('y_pred_gnb confusion matrix:\\n',confusion_matrix(y_test,y_pred_gnb),'\\n')\n",
    "print('y_pred_gnb_smote:\\n', classification_report(y_test, y_pred_gnb_smote))\n",
    "print('y_pred_gnb smote confusion matrix:\\n',confusion_matrix(y_test,y_pred_gnb_smote),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate\n",
    "rf = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "rf_smote = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "\n",
    "# fit\n",
    "rf.fit(X_train,y_train)\n",
    "rf_smote.fit(X_train_smote,y_train_smote)\n",
    "\n",
    "# predict\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_rf_smote = rf_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest Train Score:      ',rf.score(X_train,y_train))\n",
    "print('Random Forest Test Score:       ',rf.score(X_test,y_test))\n",
    "print('Random Forest Smote Train Score:',rf_smote.score(X_train,y_train))\n",
    "print('Random Forest Smote Test Score: ',rf_smote.score(X_test,y_test))\n",
    "print('\\n')\n",
    "\n",
    "print('y_pred_rf:\\n', classification_report(y_test, y_pred_rf))\n",
    "print('Random Forest confusion matrix:\\n',confusion_matrix(y_test, y_pred_rf),'\\n')\n",
    "print('y_pred_rf_smote:\\n', classification_report(y_test, y_pred_rf_smote))\n",
    "print('Random Forest confusion matrix:\\n',confusion_matrix(y_test, y_pred_rf_smote),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# instantiate\n",
    "svm = LinearSVC()\n",
    "svm_smote = LinearSVC()\n",
    "\n",
    "# fit\n",
    "svm.fit(X_train,y_train)\n",
    "svm_smote.fit(X_train_smote,y_train_smote)\n",
    "\n",
    "# predict\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "y_pred_svm_smote = svm_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM Train Score:      ',svm.score(X_train,y_train))\n",
    "print('SVM Test Score:       ',svm.score(X_test,y_test))\n",
    "print('SVM Smote Train Score:',svm_smote.score(X_train_smote,y_train_smote))\n",
    "print('SVM Smote Test Score: ',svm_smote.score(X_test,y_test))\n",
    "print('\\n')\n",
    "print('y_pred_svm:\\n', classification_report(y_test, y_pred_svm))\n",
    "print('SVM Smote confusion matrix:\\n',confusion_matrix(y_test, y_pred_svm),'\\n')\n",
    "print('y_pred_svm_smote:\\n', classification_report(y_test, y_pred_svm_smote))\n",
    "print('SVM Smote confusion matrix:\\n',confusion_matrix(y_test, y_pred_svm_smote),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# instantiate\n",
    "xgb = XGBClassifier()\n",
    "xgb_smote = XGBClassifier()\n",
    "\n",
    "# fit\n",
    "xgb.fit(X_train,y_train)\n",
    "xgb_smote.fit(X_train_smote,y_train_smote)\n",
    "\n",
    "# predict\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "y_pred_xgb_smote = xgb_smote.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('XGBoost Train Score:         ',xgb.score(X_train,y_train))\n",
    "print('XGBoost Test Score:          ',xgb.score(X_test,y_test))\n",
    "print('XGBoost Smote Train Score:   ',xgb_smote.score(X_train_smote,y_train_smote))\n",
    "print('XGBoost Smote Test Score:    ',xgb_smote.score(X_test,y_test))\n",
    "print('\\n')\n",
    "print('y_pred_xgb:\\n', classification_report(y_test, y_pred_xgb))\n",
    "print('XGBoost confusion matrix:\\n',confusion_matrix(y_test, y_pred_xgb),'\\n')\n",
    "print('y_pred_xgb_smote:\\n', classification_report(y_test, y_pred_xgb_smote))\n",
    "print('XGBoost Smote confusion matrix:\\n',confusion_matrix(y_test, y_pred_xgb_smote),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sentence = \"Where did you get that from?\"\n",
    "input_sentence = \"Are you going?\"\n",
    "\n",
    "print('Your input:   ',input_sentence)\n",
    "input_sentence = remove_punctuation(input_sentence)\n",
    "input_sentence = input_sentence.lower()\n",
    "input_sentence = [input_sentence]\n",
    "print('Pre-processed:',input_sentence[0])\n",
    "\n",
    "# Create document-term matrix using TfidfVectorizer\n",
    "input_sentence = tfidf.transform(input_sentence)\n",
    "\n",
    "# predict\n",
    "y_input_smote = logit_smote.predict(input_sentence)\n",
    "y_input = logit.predict(input_sentence)\n",
    "y_input_weight = logit_weight.predict(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logit Smote: ',bool(y_input_smote))\n",
    "print('Logit: ',bool(y_input))\n",
    "print('Logit Weighted: ',bool(y_input_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "\n",
    "Using NLTK, remove all english stop words and stem and lemmetize all verses in dataset. Then use logistic regression to see if there is a variance compared to above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def stop_punct_tokenizer(sentence):\n",
    "\n",
    "    listofwords = sentence.split(' ')\n",
    "    listofstemmed_words = []\n",
    "    \n",
    "    for word in listofwords:\n",
    "        # Remove stopwords\n",
    "        if word in ENGLISH_STOP_WORDS: continue\n",
    "        # Stem words\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        for punctuation_mark in string.punctuation:\n",
    "            # Remove punctuation and set to lower case\n",
    "            stemmed_word = stemmed_word.replace(punctuation_mark, '').lower()\n",
    "        listofstemmed_words.append(stemmed_word)\n",
    "\n",
    "    return listofstemmed_words\n",
    "\n",
    "def punct_tokenizer(sentence):\n",
    "\n",
    "    listofwords = sentence.split(' ')\n",
    "    punct_words = []\n",
    "    \n",
    "    for word in listofwords:\n",
    "        # Remove stopwords\n",
    "        if word in ENGLISH_STOP_WORDS: continue\n",
    "\n",
    "        for punctuation_mark in string.punctuation:\n",
    "            # Remove punctuation and set to lower case\n",
    "            punct_word = word.replace(punctuation_mark, '').lower()\n",
    "        punct_words.append(punct_word)\n",
    "\n",
    "    return punct_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "web_nltk_df = web_df.copy()\n",
    "\n",
    "X_nltk = web_nltk_df['script']\n",
    "y_nltk = web_nltk_df['target']\n",
    "\n",
    "# create a new train test split\n",
    "X_train_nltk, X_test_nltk, y_train_nltk, y_test_nltk = train_test_split(X_nltk,y_nltk,test_size=0.3,stratify=y,random_state=1)\n",
    "\n",
    "# remove stop words and lemmatize all words in the corpus \n",
    "vect = CountVectorizer(min_df=5, tokenizer = stop_punct_tokenizer,ngram_range=(1,3)).fit(X_train_nltk)\n",
    "# vect_punct = CountVectorizer(min_df=5, tokenizer = punct_tokenizer).fit(X_train_nltk)\n",
    "\n",
    "X_train_nltk = vect.transform(X_train_nltk)\n",
    "# X_train_nltk = vect_punct.transform(X_train_nltk)\n",
    "\n",
    "X_test_nltk = vect.transform(X_test_nltk)\n",
    "# X_test_nltk = vect_punct.transform(X_test_nltk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count features after stemming\n",
    "print('# Features With Stemming:   ',np.shape(vect.get_feature_names()))\n",
    "# print('# Features Without Stemming:',np.shape(vect_punct.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similiarly to above, we balance the question/non-question classesin the NLTK train test sets using a 1:1 ratio\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# instantiate\n",
    "smote = SMOTE(ratio=1.0)\n",
    "# fit\n",
    "X_train_nltk_smote, y_train_nltk_smote = smote.fit_sample(X_train_nltk,y_train_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate\n",
    "logit_nltk_smote = LogisticRegression(solver='lbfgs')\n",
    "logit_nltk = LogisticRegression(solver='lbfgs')\n",
    "logit_nltk_weight = LogisticRegression(solver='lbfgs',class_weight='balanced')\n",
    "\n",
    "# fit\n",
    "logit_nltk_smote.fit(X_train_nltk_smote,y_train_nltk_smote)\n",
    "logit_nltk.fit(X_train_nltk,y_train_nltk)\n",
    "logit_nltk_weight.fit(X_train_nltk,y_train_nltk)\n",
    "\n",
    "# predict\n",
    "y_pred_nltk_smote = logit_nltk_smote.predict(X_test_nltk)\n",
    "y_pred_nltk = logit_nltk.predict(X_test_nltk)\n",
    "y_pred_nltk_weight = logit_nltk_weight.predict(X_test_nltk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('NLTK Logit Train Score:         ',logit_nltk.score(X_train_nltk,y_train_nltk))\n",
    "print('NLTK Logit Test Score:          ',logit_nltk.score(X_test_nltk,y_test_nltk))\n",
    "print('NLTK Logit Train Smote Score:   ',logit_nltk_smote.score(X_train_nltk_smote,y_train_nltk_smote))\n",
    "print('NLTK Logit Test Smote Score:    ',logit_nltk_smote.score(X_test_nltk,y_test_nltk))\n",
    "print('NLTK Logit Balanced Train Score:',logit_nltk_weight.score(X_train_nltk,y_train_nltk))\n",
    "print('NLTK Logit Balanced Test Score: ',logit_nltk_weight.score(X_test_nltk,y_test_nltk))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('NLTK y_pred_smote:\\n', classification_report(y_test, y_pred_smote))\n",
    "print('NLTK y_pred_smote confusion matrix: \\n',confusion_matrix(y_test, y_pred_smote),'\\n')\n",
    "print('NLTK y_pred:\\n', classification_report(y_test, y_pred))\n",
    "print('NLTK y_pred confusion matrix: \\n',confusion_matrix(y_test, y_pred),'\\n')\n",
    "print('NLTK y_pred_weight:\\n', classification_report(y_test, y_pred_weight))\n",
    "print('NLTK y_pred_weight confusion matrix: \\n',confusion_matrix(y_test, y_pred_weight),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "__Conclusion:__ The scores are comparable to the logistic regression about without lemmetization and stemming. No need to pursue this method any further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "#Transform data\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train_nn = scaler.transform(X_train)\n",
    "# X_test_nn = scaler.transform(X_test)\n",
    "\n",
    "start = time.time()\n",
    "NN_model = MLPClassifier(hidden_layer_sizes=(500,500),solver='lbfgs')\n",
    "NN_model.fit(X_train,y_train)\n",
    "y_pred_NN = NN_model.predict(X_test)\n",
    "end = time.time()\n",
    "\n",
    "print('Execution Time:',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(NN_model)\n",
    "print('Train Score:', NN_model.score(X_train,y_train))\n",
    "print('Test Score: ', NN_model.score(X_test,y_test))\n",
    "print('Loss Score: ', NN_model.loss_)\n",
    "print('\\n')\n",
    "print('NN y_pred:\\n', classification_report(y_test, y_pred_NN))\n",
    "print('NN y_pred confusion matrix: \\n',confusion_matrix(y_test, y_pred_NN),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ The following is find the optimal number of epochs to train the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# for storing accuracies and score\n",
    "epoch_train_accuracies = []\n",
    "epoch_test_accuracies = []\n",
    "num_epoch_times = []\n",
    "epoch_loss_scores = []\n",
    "\n",
    "# controls the number of epochs and step sizes\n",
    "num_epochs = 101\n",
    "epoch_step_size = 1\n",
    "\n",
    "# loop through several neural network models\n",
    "for i in np.arange(1,num_epochs,epoch_step_size):\n",
    "    # for timing the execution\n",
    "    start = time.time()\n",
    "    # instantiate and fit neural network, controlling the number of epochs using max_iter=i\n",
    "    # where i is controlled by the epoch_step_size up to a max of num_epochs\n",
    "    NN_model = MLPClassifier(hidden_layer_sizes=500,solver='lbfgs',max_iter=i)\n",
    "    NN_model.fit(X_train,y_train)\n",
    "    \n",
    "    # capture local accuracy and scores for each model\n",
    "    train_accuracy = NN_model.score(X_train,y_train)\n",
    "    test_accuracy = NN_model.score(X_test,y_test)\n",
    "    epoch_loss_scores.append(NN_model.loss_)\n",
    "    \n",
    "    # store local accuracy and scores for each model in global variable\n",
    "    epoch_train_accuracies.append(train_accuracy)\n",
    "    epoch_test_accuracies.append(test_accuracy)\n",
    "        \n",
    "    end = time.time()\n",
    "    num_epoch_times.append(end-start)\n",
    "    print('Epochs =',i,'Execution Time: ', end-start, 'seconds')\n",
    "    print('Train Accuracy: ', train_accuracy)\n",
    "    print('Test Accuracy:  ', test_accuracy)\n",
    "    print('Loss Score:     ', NN_model.loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(num_epoch_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ The following is find the optimal train range and step size to train the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# for storing accuracies globally\n",
    "neuron_train_accuracies = []\n",
    "neuron_test_accuracies = []\n",
    "neuron_size_hidden_layer_times = []\n",
    "neuron_loss_scores = []\n",
    "\n",
    "train_range = 501\n",
    "step_size = 100\n",
    "\n",
    "# loop through several neural network models\n",
    "for j in np.arange(1,train_range,step_size):\n",
    "    start = time.time()\n",
    "    # instantiate and fit neural network, controlling the hidden layer size using hidden_layer_sizes=j\n",
    "    # where j is controlled by the step_size up to a max of train_range\n",
    "    NN_model = MLPClassifier(hidden_layer_sizes=j,solver='lbfgs')\n",
    "    NN_model.fit(X_train,y_train)\n",
    "\n",
    "    # capture local accuracy and loss scores\n",
    "    neuron_train_accuracy = NN_model.score(X_train,y_train)\n",
    "    neuron_test_accuracy = NN_model.score(X_test,y_test)\n",
    "    neuron_loss_score = NN_model.loss_\n",
    "    \n",
    "    # store local accuracy and loss scores in global variable\n",
    "    neuron_train_accuracies.append(neuron_train_accuracy)\n",
    "    neuron_test_accuracies.append(neuron_test_accuracy)\n",
    "    neuron_loss_scores.append(neuron_loss_score)\n",
    "        \n",
    "    end = time.time()\n",
    "    neuron_size_hidden_layer_times.append(end-start)\n",
    "    print('Hidden Layer = (',j,'), Execution Time: ', end-start, 'seconds')\n",
    "    print('Train Accuracy: ', neuron_train_accuracy)\n",
    "    print('Test Accuracy:  ', neuron_test_accuracy)\n",
    "    print('Loss Score:     ', neuron_loss_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(neuron_size_hidden_layer_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The best classifier was the Logistic Regression, with the following metrics/scores:\n",
    "\n",
    "Logit Train Score:       0.9274289916352606 <br>\n",
    "Logit Test Score:        0.9204289544235925 <br>\n",
    "Logit Train Smote Score: 0.9209275565339203 <br>\n",
    "Logit Test Smote Score:  0.8441823056300268 <br>\n",
    "Logit Balanced Train Score:       0.87990624138248 <br>\n",
    "Logit Balanced Test Score:        0.828739946380697 <br>\n",
    "\n",
    "\n",
    "__y_pred_smote:__<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; precision &emsp;    recall &emsp;&emsp;  f1-score &emsp;   support &emsp;<br>\n",
    "\n",
    "           0       0.95      0.87      0.91      8567 \n",
    "           1       0.27      0.52      0.35       758\n",
    "\n",
    "    accuracy                           0.84      9325\n",
    "   macro avg &emsp;&emsp;&emsp;&emsp;&emsp;       0.61&emsp;      0.70&emsp;      0.63&emsp;      9325<br>\n",
    "weighted avg &emsp;&emsp;&emsp;&emsp;       0.90&emsp;      0.84&emsp;      0.87&emsp;      9325<br>\n",
    "\n",
    "__y_pred_smote confusion matrix:__<br>\n",
    " [[7480 1087] <br>\n",
    " [ 366  392]] <br>\n",
    "\n",
    "__y_pred_weight:__ <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; precision &emsp;    recall &emsp;&emsp;  f1-score &emsp;   support &emsp;<br>\n",
    "\n",
    "           0       0.96      0.85      0.90      8567\n",
    "           1       0.26      0.58      0.36       758\n",
    "\n",
    "    accuracy                           0.83      9325\n",
    "macro avg &emsp;&emsp;&emsp;&emsp;&emsp;       0.61&emsp;      0.72&emsp;      0.63&emsp;      9325 <br>\n",
    "weighted avg &emsp;&emsp;&emsp;&emsp;       0.90&emsp;      0.83&emsp;      0.86&emsp;      9325 <br>\n",
    "\n",
    "__y_pred_weight confusion matrix:__ <br>\n",
    " [[7287 1280] <br>\n",
    " [ 317  441]] <br>\n",
    "\n",
    "__Despite__ the accuracy being high, it is not the best indicator of the success of a classifier in this particular case. The primary metric/score used to determine the best classifier was the precision, recall and f1-score. <br> \n",
    "\n",
    "Precision is defined as: \n",
    "\\begin{equation*}\n",
    "\\frac{True Positive}{True Positive+False Positive}\n",
    "\\end{equation*}\n",
    "\n",
    "Recall is defined as:\n",
    "\\begin{equation*}\n",
    "\\frac{True Positive}{True Positive+False Negative}\n",
    "\\end{equation*}\n",
    "\n",
    "F1-Score is defined as:\n",
    "\\begin{equation*}\n",
    "2 \\times \\frac{Precision*Recall}{Precision+Recall}\n",
    "\\end{equation*}\n",
    "\n",
    "Recall was used as we are primarily interested in the correct identification of a question. That being said, the F1 Score is also a good measure to use because we seek a balance between Precision and Recall AND there is an uneven class distribution (larger number of Actual Negatives), which is apparent in this particular dataset. The confusion matrix is also used as a visual grid summary that displays the predict vs actual true negative/positives and false negative/positives.\n",
    "\n",
    "# End Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
